---
layout: post
tile:  "storm调优"
date:  2015-11-13 11:14:48
categories: storm 
excerpt: storm调优
---

* content
{:toc}



本文从2个方面讨论storm的调优，第一个是集群的调优，第二个是运行在集群中的拓扑的调优，这部分还包括了使用storm-kafka从kafka中读取消息的调优。

#一、集群调优

##1、netty的调优

netty的配置项主要包括以下几个：
	
	storm.messaging.netty.server_worker_threads: 1
	storm.messaging.netty.client_worker_threads: 1
	storm.messaging.netty.buffer_size: 5242880 #5MB buffer
	# Since nimbus.task.launch.secs and supervisor.worker.start.timeout.secs are 120, other workers should also wait at least that long before giving up on connecting to the other worker. The reconnection period need also be bigger than storm.zookeeper.session.timeout(default is 20s), so that we can abort the reconnection when the target worker is dead.
	storm.messaging.netty.max_retries: 300
	storm.messaging.netty.max_wait_ms: 1000
	storm.messaging.netty.min_wait_ms: 100
	
	# If the Netty messaging layer is busy(netty internal buffer not writable), the Netty client will try to batch message as more as possible up to the size of storm.messaging.netty.transfer.batch.size bytes, otherwise it will try to flush message as soon as possible to reduce latency.
	storm.messaging.netty.transfer.batch.size: 262144
	# Sets the backlog value to specify when the channel binds to a local address
	storm.messaging.netty.socket.backlog: 500
	
	# By default, the Netty SASL authentication is set to false.  Users can override and set it true for a specific topology.
	storm.messaging.netty.authentication: false


#二、拓扑调优

##1、使用组件的并行度代替线程池
在storm中，我们可以很方便的调整spout/bolt的并行度，即使启动拓扑时设置不合理，也可以使用rebanlance命令进行动态调整。
但有些人可能会在一个spout/bolt组件的task内部启动一个线程池，这些线程池所在的task会比其余task消耗更多的资源，因此这些task所在的worker会消耗较多的资源，有可能影响其它拓扑的正常执行。
因此，应该使用组件自身的并行度来代替线程池，因为这些并行度会被合理分配到不同的worker中去。除此之外，还可以使用CGroup等技术进行资源的控制。

##2、不要在spout中处理耗时的操作
在storm中，spout是单线程的。如果nextTuple方法非常耗时，某个消息被成功执行完毕后，acker会给spout发送消息，spout若无法及时消费，则有可能导致 ack消息被丢弃，然后spout认为执行失败了。
在jstorm中将spout分成了3个线程，分别执行nextTuple, fail, ack方法。

##3、fieldsGrouping的数据均衡性
fieldsGrouping根据某个field的值进行分组，以userId为例，如果一个组件以userId的值作为分组，则具有相同userId的值会被发送到同一个task。如果某些userId的数据量特别大，会导致这接收这些数据的task负载特别高，从而导致数据均衡出现问题。
因此必须合理选择field的值，或者更换分组策略。

##4、优先使用localOrShuffleGrouping代替shuffleGrouping
localOrShuffleGrouping是指如果task发送消息给目标task时，发现同一个worker中有目标task，则优先发送到这个task；如果没有，则进行shuffle，随机选取一个目标task。
localOrShuffleGrouping其实是对shuffleGrouping的一个优化，因为消除了网络开销和序列化操作。


##5、设置合理的MaxSpoutPending
在启用了ack的情况下，spout中有个RotatingMap来保存spout已经发送出去，但未收到ack结果的消息。RotatingMap最大的大小为p*num-task，其中num-task就是spout的task数量，而p为topology.max.spout.pending的值，也可以通过setMaxSpoutPending来指定，是指每个task最多已经发送出去但未被ack的消息数量。

若设置过小，则task的处理能力未充分应用，不能达到最佳的吞吐量。若设置过大，则消费过多的内存，还有可能spout的消息不能及时处理，从而导致fail的出现。

1、spout的Execute latency（执行nextTuple的时间）为17ms，因此理论上每秒每个spout task的最大发送速度是60个tuple。
2、一个tuple的处理时长约为200ms(topo的complete latency）
3、200ms内有大约有60*0.2=12个tuple被发送。
4、因此MaxSpoutPenging被设置为12较为合理。

小结：1/Execute latency*complete latency，即使用topo的complete latency除以Execute latency即可，但实际上不应该考虑如此极端的情况，以避免过多的fail出现，所以可以设置为上述值除以1.5左右。

默认值为1，需要改为合理的值。对trident是否适用？？

	//任何时刻中，一个spout task最多可以同时处理的tuple数量，即已经emite,但未acked的tuple数量。默认为1
        Number active = (Number) conf.get(Config.TOPOLOGY_MAX_SPOUT_PENDING);
        if(active==null) {
            _maxTransactionActive = 1;
        } else {
            _maxTransactionActive = active.intValue();
        }
