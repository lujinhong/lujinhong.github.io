---
layout: post
tile:  "spark RDD详解（未发）"
date:  2015-12-13 22:15:20
categories: spark 
excerpt: spark RDD详解（未发）
---

* content
{:toc}



#一、基础
##（一）什么是RDD

A Resilient Distributed Dataset (RDD), the basic abstraction in Spark. Represents an immutable, partitioned collection of elements that can be operated on in parallel. 

RDD是spark最基本的抽象概念，spark中的所有数据均通过RDD的形式进行组织。RDD是弹性的，自动容错的，分区的，只读的记录集合。

##（二）RDD的适用范围
RDD尤其适用于迭代式的数据处理，如机器学习等。但它不适合那些异步更新共享状态的应用，例如web爬虫。

##（三）一些特性
1、 在部分分区数据丢失时，spark可以通过依赖关系重新计算丢失的分区数据，而不是对RDD的所有分区进行重算。
2、用户可以在创建RDD时指定RDD的分区数量，如果没有指定，那么就会采用默认值，即程序分区到的CPU core数目。对于HDFS，每个block会分配一个分区。对于由父RDD生成的子RDD，其分区数量与父RDD相同，或者在transformation中显式指定。详见spark调优那篇文章。

##（四）RDD的创建
RDD有2种创建方式

###1、由一个已经存在的scala集合创建

	val rdd = sc.paralellize(List(1,2,3,4))
一般只在试验性代码中使用，生产环境不大可能用到。
###2、由外部存储系统的数据创建
比如本地文件，HDFS, HBASE等，常用textFile方法

	val rdd = sc.textFile("hdfs:///tmp/myfile.txt")

##（五）RDD的操作
RDD有2种操作：transformation 与 action，详见RDD的transformation 与 action那篇文章。

#二、RDD的缓存
对于一个经常被使用的RDD或者计算代价较大的RDD，将其缓存下来，会大大的提高处理速度。
##（一）缓存方式
persist()是标准的缓存方法
cache()是其简化方法，当只使用内存作缓存时使用。

##（二）缓存级别【待补充】

#三、窄依赖与宽依赖&stage的划分依据【待补充代码】
RDD根据对父RDD的依赖关系，可分为窄依赖与宽依赖2种。
主要的区分之处在于父RDD的分区被多少个子RDD分区所依赖，如果一个就为窄依赖，多个则为宽依赖。

##（一）窄依赖
###1、OneToOneDependency
一对一依赖，即每个子RDD的分区的与父RDD的分区一一对应。

###2、RangeDependency
子RDD中的每个分区依赖于父RDD的几个分区，而父RDD的每个分区仅补一个子RDD分区所依赖，即多对一的关系。它仅仅被UnionRDD所使用。


##（二）宽依赖
宽依赖只有一种：shuffleDependency，即子RDD依赖于父RDD的所有分区，父RDD的分每个区被所有子RDD的分区所依赖。


###（三）stage的划分
DAG根据宽依赖来划分stage，每个宽依赖的处理均会一个stage的划分点。同一个stage中的多个操作会在一个task中完成。因为子RDD的分区仅依赖于父RDD的一个分区，因此这些步骤可以串行执行。


#4、RDD的源码【待补充】
