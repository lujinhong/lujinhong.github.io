---
layout: post
tile:  "kafka集群编程指南"
date:  2015-08-01 23:47:27
categories: kafka 大数据 
excerpt: kafka集群编程指南
---

* content
{:toc}




JAVA API：http://kafka.apache.org/082/javadoc/index.html

#一、概述

##（一）主要内容
本文主要介绍了以下4部分内容

（1）向kafka集群发送消息的producer JAVA API

（2）从kafka集群消费消息的consumer JAVA API

（3）使用storm从kafka集群中消费消息

（4）使用spark streaming从kafka集群中消费消息

##（二）关于scala与java的说明
由于kafka本身是用scala语言写的，但大多使用kafka集群的用户都习惯使用java语言，因此，kafka使用scala语言写了一个java版本的API，目前它同时支持producer与consumer。

从0.8.2版本开始，kafka使用java语言重写了producer API，并计划于0.8.3（官方说下一个版本，没有具体说哪个）使用java语言重写consumer API。官方推荐使用新producer API代替原有的scala语言写的API。

总结：

（1）kafka_0.8.2有2个版本producer API，分别是scala版本与java版本，前者放到源码的core目录下，后者放在源码的client目录下。官方推荐使用java语言版本，scala语言版本不再更新。

（2）kafka_0.8.2目前只有scala版本的consumer API，计划于下一个版本中增加java版本的。

#二、producer的API

此部分先简单介绍一下scala版本的API，然后再深入介绍java版本的API。

##（一）scala版本（deprecated）
基本步骤为创建producer——>使用producer发送消息——>关闭producer
（1）创建producer
		
	Properties props = new Properties();
	props.put("serializer.class", "kafka.serializer.StringEncoder");
	props.put("metadata.broker.list","192.168.172.117:9092");
	props.put("producer.type", "sync");
	Producer<Integer, String> producer = new Producer<Integer, String>(new ProducerConfig(props));

（2）使用producer发送消息

	producer.send(new KeyedMessage<Integer, String>("topic", "message”);
2个参数分别为topic名称和发送的消息内容，均为String类型。

（3）关闭producer

	producer.close();

##（二）java版本
待补充
#三、consumer的API 
待补充
##（一）high level consummer
提供了更高层的抽象, 详细请参考：https://cwiki.apache.org/confluence/display/KAFKA/Consumer+Group+Example
 
##（二）simple（low level) consumer
对于大部分的应用来说，high level的api已经足够完成功能。如果需要更底层的功能（如定义启动时的offset等），则需要使用simple（low level) consumer。
详细请参考：https://cwiki.apache.org/confluence/display/KAFKA/0.8.0+SimpleConsumer+Example
For most applications, the high level consumer Api is good enough. Some applications want features not exposed to the high level consumer yet (e.g., set initial offset when restarting the consumer). They can instead use our low level SimpleConsumer Api. The logic will be a bit more complicated and you can follow the example in here. 
#四、使用storm从kafka集群中消费消息
一个简单示例：
Core Spout
		
	BrokerHosts hosts = new ZkHosts(zkConnString);
	SpoutConfig spoutConfig = new SpoutConfig(hosts, topicName, "/" + topicName, UUID.randomUUID().toString());
	spoutConfig.scheme = new SchemeAsMultiScheme(new StringScheme());
	KafkaSpout kafkaSpout = new KafkaSpout(spoutConfig);

Trident Spout
		
	TridentTopology topology = new TridentTopology();
	BrokerHosts zk = new ZkHosts("localhost");
	TridentKafkaConfig spoutConf = new TridentKafkaConfig(zk, "test-topic");
	spoutConf.scheme = new SchemeAsMultiScheme(new StringScheme());
	OpaqueTridentKafkaSpout spout = new OpaqueTridentKafkaSpout(spoutConf);

详细请参考storm-kafka编程指南

#五、使用spark streaming从kafka集群中消费消息
1、定义你所需要完成的功能

这里以日志的过滤为例：
		
	public class FiltersFuntion implements Function<Tuple2<String, String>, String>,akka.japi.Function<Tuple2<String, String>, String>

关键是覆盖Function中的call()方法，它定义了对每一个消息所作的处理

		@Override
	    public String call(Tuple2<String, String> v1) throws Exception {

2、定义应用的结构
		
	public final class JavaKafkaWordCount {
	  private static final Pattern SPACE = Pattern.compile(" ");
	
	  private JavaKafkaWordCount() {
	  }
	
	  public static void main(String[] args) {
	    if (args.length < 4) {
	      System.err.println("Usage: JavaKafkaWordCount <zkQuorum> <group> <topics> <numThreads>");
	      System.exit(1);
	    }
	
	    SparkConf sparkConf = new SparkConf().setAppName("ljh_JavaKafkaWordCount");
	    // Create the context with a 1 second batch size
	    JavaStreamingContext jssc = new JavaStreamingContext(sparkConf, new Duration(2000));
	
	    int numThreads = Integer.parseInt(args[3]);
	    Map<String, Integer> topicMap = new HashMap<String, Integer>();
	    String[] topics = args[2].split(",");
	    for (String topic: topics) {
	      topicMap.put(topic, numThreads);
	    }
	
	    JavaPairReceiverInputDStream<String, String> messages =
	            KafkaUtils.createStream(jssc, args[0], args[1], topicMap);
	
	    JavaDStream<String> lines = messages.map(new FiltersFuntion() );
	
	    JavaDStream<String> words = lines.flatMap(new FlatMapFunction<String, String>() {
	      @Override
	      public Iterable<String> call(String x) {
	        return Lists.newArrayList(SPACE.split(x));
	      }
	    });
	
	    JavaPairDStream<String, Integer> wordCounts = words.mapToPair(
	      new PairFunction<String, String, Integer>() {
	        @Override
	        public Tuple2<String, Integer> call(String s) {
	          return new Tuple2<String, Integer>(s, 1);
	        }
	      }).reduceByKey(new Function2<Integer, Integer, Integer>() {
	        @Override
	        public Integer call(Integer i1, Integer i2) {
	          return i1 + i2;
	        }
	      });
	
	    wordCounts.print();
	    jssc.start();
	    jssc.awaitTermination();
	  }
	}


#六、与hadoop的集成
 
通过一个叫camus的第三方项目实现，请谨慎使用。
Providing a horizontally scalable solution for aggregating and loading data into Hadoop was one of our basic use cases. To support this use case, we provide a Hadoop-based consumer which spawns off many map tasks to pull data from the Kafka cluster in parallel. This provides extremely fast pull-based Hadoop data load capabilities (we were able to fully saturate the network with only a handful of Kafka servers). 

 详细请参考项目：https://github.com/linkedin/camus/
