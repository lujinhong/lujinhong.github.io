<!DOCTYPE html><html><head><title>storm-kafka编程指南</title><meta charset='utf-8'><link href='https://dn-maxiang.qbox.me/res-min/themes/marxico.css' rel='stylesheet'></head><body><div id='preview-contents' class='note-content'>
                        <div id="wmd-preview" class="preview-content"></div>
                    <div id="wmd-preview-section-275" class="wmd-preview-section preview-content">

</div><div id="wmd-preview-section-276" class="wmd-preview-section preview-content">

<h1 id="storm-kafka编程指南">storm-kafka编程指南</h1>

<p></p>

<div><div class="toc"><div class="toc">
<ul>
<li><a href="#storm-kafka编程指南">storm-kafka编程指南</a></li>
<li><a href="#一原理及关键步骤介绍">一、原理及关键步骤介绍</a><ul>
<li><a href="#一使用storm-kafka的关键步骤">（一）使用storm-kafka的关键步骤</a><ul>
<li><a href="#1创建zkhosts">1、创建ZkHosts</a></li>
<li><a href="#2创建kafkaconfig">2、创建KafkaConfig</a></li>
<li><a href="#3设置multischeme">3、设置MultiScheme</a></li>
<li><a href="#4创建spout">4、创建Spout</a></li>
<li><a href="#5建立拓扑">5、建立拓扑：</a></li>
</ul>
</li>
<li><a href="#二当拓扑出错时如何从上一次的kafka位置继续处理消息">（二）当拓扑出错时，如何从上一次的kafka位置继续处理消息</a></li>
<li><a href="#三结果写回kafka">（三）结果写回kafka</a></li>
</ul>
</li>
<li><a href="#二完整示例">二、完整示例</a><ul>
<li><a href="#一简介">（一）简介</a></li>
<li><a href="#二单词拆分">（二）单词拆分</a></li>
<li><a href="#三定义拓扑行为">（三）定义拓扑行为</a><ul>
<li><a href="#1定义kafka的相关配置">1、定义kafka的相关配置</a></li>
<li><a href="#2定义拓扑进行单词统计后写入一个分布式内存中">2、定义拓扑，进行单词统计后，写入一个分布式内存中。</a></li>
<li><a href="#3从分布式内存中读取结果并进行输出">3、从分布式内存中读取结果并进行输出</a></li>
</ul>
</li>
<li><a href="#四state定义">（四）state定义</a><ul>
<li><a href="#1factory类">1、Factory类</a></li>
<li><a href="#3state类">3、State类</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>
</div>

</div><div id="wmd-preview-section-277" class="wmd-preview-section preview-content">

<h1 id="一原理及关键步骤介绍">一、原理及关键步骤介绍</h1>

<p>storm中的storm-kafka组件提供了storm与kafka交互的所需的所有功能，请参考其官方文档：<a href="https://github.com/apache/storm/tree/master/external/storm-kafka#brokerhosts" target="_blank">https://github.com/apache/storm/tree/master/external/storm-kafka#brokerhosts</a></p>

</div><div id="wmd-preview-section-278" class="wmd-preview-section preview-content">

<h2 id="一使用storm-kafka的关键步骤">（一）使用storm-kafka的关键步骤</h2>

</div><div id="wmd-preview-section-279" class="wmd-preview-section preview-content">

<h3 id="1创建zkhosts">1、创建ZkHosts</h3>

<p>当storm从kafka中读取某个topic的消息时，需要知道这个topic有多少个分区，以及这些分区放在哪个kafka节点(broker)上，ZkHosts就是用于这个功能。 <br>
创建zkHosts有2种形式</p>

</div><div id="wmd-preview-section-280" class="wmd-preview-section preview-content">

<pre class="prettyprint hljs-dark"><code class="hljs stylus">   public <span class="hljs-function"><span class="hljs-title">ZkHosts</span><span class="hljs-params">(String brokerZkStr, String brokerZkPath)</span></span> 
   public <span class="hljs-function"><span class="hljs-title">ZkHosts</span><span class="hljs-params">(String brokerZkStr)</span></span></code></pre>

<p>（1）默认情况下，zk信息被放到/brokers中，此时可以使用第2种方式： <br>
new ZkHosts(“192.168.172.117:2181,192.168.172.98:2181,192.168.172.111:2181,192.168.172.114:2181,192.168.172.116:2181”);</p>

<p>（2）若zk信息被放置在/kafka/brokers中（我们的集群就是这种情形），则可以使用： <br>
 publicZkHosts(“192.168.172.117:2181,192.168.172.98:2181,192.168.172.111:2181,192.168.172.114:2181,192.168.172.116:2181”,“/kafka”) </p>

<p>或者直接：</p>

</div><div id="wmd-preview-section-281" class="wmd-preview-section preview-content">

<pre class="prettyprint hljs-dark"><code class="hljs 1c"> new ZkHosts(<span class="hljs-string">"192.168.172.117:2181,192.168.172.98:2181,192.168.172.111:2181,192.168.172.114:2181,192.168.172.116:2181/kafka”)</span></code></pre>

<p>默认情况下，每60秒去读取一次kafka的分区信息，可以通过修改host.refreshFreqSecs来设置。</p>

<p>（3）除了使用ZkHosts来读取分析信息外，storm-kafka还提供了一种静态指定的方法（不推荐此方法），如：</p>

</div><div id="wmd-preview-section-282" class="wmd-preview-section preview-content">

<pre class="prettyprint hljs-dark"><code class="hljs actionscript">    Broker brokerForPartition0 = <span class="hljs-keyword">new</span> Broker(<span class="hljs-string">"localhost"</span>);<span class="hljs-comment">//localhost:9092</span>
    Broker brokerForPartition1 = <span class="hljs-keyword">new</span> Broker(<span class="hljs-string">"localhost"</span>, <span class="hljs-number">9092</span>);<span class="hljs-comment">//localhost:9092 but we specified the port explicitly</span>
    Broker brokerForPartition2 = <span class="hljs-keyword">new</span> Broker(<span class="hljs-string">"localhost:9092"</span>);<span class="hljs-comment">//localhost:9092 specified as one string.</span>
    GlobalPartitionInformation partitionInfo = <span class="hljs-keyword">new</span> GlobalPartitionInformation();
    partitionInfo.addPartition(<span class="hljs-number">0</span>, brokerForPartition0);<span class="hljs-comment">//mapping form partition 0 to brokerForPartition0</span>
    partitionInfo.addPartition(<span class="hljs-number">1</span>, brokerForPartition1);<span class="hljs-comment">//mapping form partition 1 to brokerForPartition1</span>
    partitionInfo.addPartition(<span class="hljs-number">2</span>, brokerForPartition2);<span class="hljs-comment">//mapping form partition 2 to brokerForPartition2</span>
    StaticHosts hosts = <span class="hljs-keyword">new</span> StaticHosts(partitionInfo);</code></pre>

<p>由此可以看出，ZkHosts完成的功能就是指定了从哪个kafka节点读取某个topic的哪个分区。</p>

</div><div id="wmd-preview-section-283" class="wmd-preview-section preview-content">

<h3 id="2创建kafkaconfig">2、创建KafkaConfig</h3>

<p>(1)有2种方式创建KafkaConfig <br>
   public KafkaConfig(BrokerHosts hosts, String topic) <br>
   public KafkaConfig(BrokerHosts hosts, String topic, String clientId) <br>
BrokerHosts就是上面创建的实例，topic就是要订阅的topic名称，clientId用于指定存放当前topic consumer的offset的位置，这个id 应该是唯一的，否则多个拓扑会引起冲突。 <br>
事实上，trident的offset并不保存在这个位置，见下面介绍。 <br>
真正使用时，有2种扩展，分别用于一般的storm以及trident。 <br>
（2）core storm <br>
Spoutconfig is an extension of KafkaConfig that supports additional fields with ZooKeeper connection info and for controlling behavior specific to KafkaSpout. The Zkroot will be used as root to store your consumer’s offset. The id should uniquely identify your spout. <br>
public SpoutConfig(BrokerHosts hosts, String topic, String zkRoot, String id); <br>
public SpoutConfig(BrokerHosts hosts, String topic, String id); <br>
In addition to these parameters, SpoutConfig contains the following fields that control how KafkaSpout behaves:</p>

</div><div id="wmd-preview-section-284" class="wmd-preview-section preview-content">

<pre class="prettyprint hljs-dark"><code class="hljs aspectj"><span class="hljs-comment">// setting for how often to save the current kafka offset to ZooKeeper</span>
    <span class="hljs-keyword">public</span> <span class="hljs-keyword">long</span> stateUpdateIntervalMs = <span class="hljs-number">2000</span>;

    <span class="hljs-comment">// Exponential back-off retry settings.  These are used when retrying messages after a bolt</span>
    <span class="hljs-comment">// calls OutputCollector.fail().</span>
    <span class="hljs-comment">// Note: be sure to set backtype.storm.Config.MESSAGE_TIMEOUT_SECS appropriately to prevent</span>
    <span class="hljs-comment">// resubmitting the message while still retrying.</span>
    <span class="hljs-keyword">public</span> <span class="hljs-keyword">long</span> retryInitialDelayMs = <span class="hljs-number">0</span>;
    <span class="hljs-keyword">public</span> <span class="hljs-keyword">double</span> retryDelayMultiplier = <span class="hljs-number">1.0</span>;
    <span class="hljs-keyword">public</span> <span class="hljs-keyword">long</span> retryDelayMaxMs = <span class="hljs-number">60</span> * <span class="hljs-number">1000</span>;</code></pre>

<p>KafkaSpout 只接受 SpoutConfig作为参数</p>

<p>（3）TridentKafkaConfig，TridentKafkaEmitter只接受TridentKafkaConfig使用参数 <br>
trident消费kafka的offset位置是在建立拓扑中指定，如：</p>

</div><div id="wmd-preview-section-285" class="wmd-preview-section preview-content">

<pre class="prettyprint hljs-dark"><code class="hljs stylus">topology.<span class="hljs-function"><span class="hljs-title">newStream</span><span class="hljs-params">(test, kafkaSpout)</span></span>.</code></pre>

<p>则offset的位置为：</p>

</div><div id="wmd-preview-section-286" class="wmd-preview-section preview-content">

<pre class="prettyprint hljs-dark"><code class="hljs gradle"><span class="hljs-regexp">/transactional/</span>test<span class="hljs-regexp">/coordinator/</span>currtx</code></pre>

<p>（4）KafkaConfig的一些默认参数 </p>

</div><div id="wmd-preview-section-287" class="wmd-preview-section preview-content">

<pre class="prettyprint hljs-dark"><code class="hljs gradle">    <span class="hljs-keyword">public</span> <span class="hljs-keyword">int</span> fetchSizeBytes = <span class="hljs-number">1024</span> * <span class="hljs-number">1024</span>;
    <span class="hljs-keyword">public</span> <span class="hljs-keyword">int</span> socketTimeoutMs = <span class="hljs-number">10000</span>;
    <span class="hljs-keyword">public</span> <span class="hljs-keyword">int</span> fetchMaxWait = <span class="hljs-number">10000</span>;
    <span class="hljs-keyword">public</span> <span class="hljs-keyword">int</span> bufferSizeBytes = <span class="hljs-number">1024</span> * <span class="hljs-number">1024</span>;
    <span class="hljs-keyword">public</span> MultiScheme scheme = <span class="hljs-keyword">new</span> RawMultiScheme();
    <span class="hljs-keyword">public</span> <span class="hljs-keyword">boolean</span> forceFromStart = <span class="hljs-keyword">false</span>;
    <span class="hljs-keyword">public</span> <span class="hljs-keyword">long</span> startOffsetTime = kafka.api.OffsetRequest.EarliestTime();
    <span class="hljs-keyword">public</span> <span class="hljs-keyword">long</span> maxOffsetBehind = <span class="hljs-keyword">Long</span>.MAX_VALUE;
    <span class="hljs-keyword">public</span> <span class="hljs-keyword">boolean</span> useStartOffsetTimeIfOffsetOutOfRange = <span class="hljs-keyword">true</span>;
    <span class="hljs-keyword">public</span> <span class="hljs-keyword">int</span> metricsTimeBucketSizeInSecs = <span class="hljs-number">60</span>;</code></pre>

<p>可以通过以下方式修改：</p>

</div><div id="wmd-preview-section-288" class="wmd-preview-section preview-content">

<pre class="prettyprint hljs-dark"><code class="hljs ocaml">kafkaConfig.scheme =<span class="hljs-keyword">new</span> <span class="hljs-type">SchemeAsMultiScheme</span>(<span class="hljs-keyword">new</span> <span class="hljs-type">StringScheme</span><span class="hljs-literal">()</span>);</code></pre>

</div><div id="wmd-preview-section-289" class="wmd-preview-section preview-content">

<h3 id="3设置multischeme">3、设置MultiScheme</h3>

<p>MultiScheme用于指定如何处理从kafka中读取到的字节，同时它用于控制输出字段名称。</p>

</div><div id="wmd-preview-section-290" class="wmd-preview-section preview-content">

<pre class="prettyprint hljs-dark"><code class="hljs aspectj"> <span class="hljs-keyword">public</span> Iterable&lt;List&lt;Object&gt;&gt; deserialize(<span class="hljs-keyword">byte</span>[] ser);
 <span class="hljs-keyword">public</span> <span class="hljs-function">Fields <span class="hljs-title">getOutputFields</span><span class="hljs-params">()</span></span>;</code></pre>

<p>默认情况下，RawMultiScheme读取一个字段并返回一个字节，而发射的字段名称为bytes。 <br>
可以通过SchemeAsMultiScheme和 KeyValueSchemeAsMultiScheme改变这种默认行为：</p>

</div><div id="wmd-preview-section-291" class="wmd-preview-section preview-content">

<pre class="prettyprint hljs-dark"><code class="hljs ocaml"> kafkaConfig.scheme =<span class="hljs-keyword">new</span> <span class="hljs-type">SchemeAsMultiScheme</span>(<span class="hljs-keyword">new</span> <span class="hljs-type">StringScheme</span><span class="hljs-literal">()</span>);</code></pre>

<p>上面的语句指定了将字节转化为字符。 <br>
同时建立拓扑时：</p>

</div><div id="wmd-preview-section-292" class="wmd-preview-section preview-content">

<pre class="prettyprint hljs-dark"><code class="hljs python">topology.newStream(“test<span class="hljs-string">",kafkaSpout).each(new Fields("</span>st<span class="hljs-string">r"),new FilterFunction(),new Fields("</span>word”))….</code></pre>

<p>会指定发射的字段名称为str。</p>

</div><div id="wmd-preview-section-293" class="wmd-preview-section preview-content">

<h3 id="4创建spout">4、创建Spout</h3>

<p>(1)core storm</p>

</div><div id="wmd-preview-section-294" class="wmd-preview-section preview-content">

<pre class="prettyprint hljs-dark"><code class="hljs actionscript">KafkaSpout kafkaSpout = <span class="hljs-keyword">new</span> KafkaSpout(spoutConfig);</code></pre>

<p>(2)trident</p>

</div><div id="wmd-preview-section-295" class="wmd-preview-section preview-content">

<pre class="prettyprint hljs-dark"><code class="hljs actionscript"> OpaqueTridentKafkaSpoutkafkaSpout = <span class="hljs-keyword">new</span> OpaqueTridentKafkaSpout(kafkaConfig);</code></pre>

</div><div id="wmd-preview-section-296" class="wmd-preview-section preview-content">

<h3 id="5建立拓扑">5、建立拓扑：</h3>

<p>(1)core-storm</p>

</div><div id="wmd-preview-section-297" class="wmd-preview-section preview-content">

<pre class="prettyprint hljs-dark"><code class="hljs actionscript">builder.setSpout(<span class="hljs-string">"kafka-reader"</span>,<span class="hljs-keyword">new</span> KafkaSpout(spoutConf),<span class="hljs-number">12</span>);</code></pre>

<p>kafka-reader指定了spout的名称，12指定了并行度。</p>

<p>(2)trident</p>

</div><div id="wmd-preview-section-298" class="wmd-preview-section preview-content">

<pre class="prettyprint hljs-dark"><code class="hljs python">topology.newStream(“test<span class="hljs-string">", kafkaSpout). each(new Fields("</span>st<span class="hljs-string">r"), new FilterFunction(),new Fields("</span>word”))….</code></pre>

<p>test指定了放置offset的位置，也就是txid的位置。str指定了spout发射字段的名称。</p>

<p>完整示例： <br>
Core Spout</p>

</div><div id="wmd-preview-section-299" class="wmd-preview-section preview-content">

<pre class="prettyprint hljs-dark"><code class="hljs ocaml"><span class="hljs-type">BrokerHosts</span> hosts = <span class="hljs-keyword">new</span> <span class="hljs-type">ZkHosts</span>(zkConnString);
<span class="hljs-type">SpoutConfig</span> spoutConfig = <span class="hljs-keyword">new</span> <span class="hljs-type">SpoutConfig</span>(hosts, topicName, <span class="hljs-string">"/"</span> + topicName, <span class="hljs-type">UUID</span>.randomUUID<span class="hljs-literal">()</span>.toString<span class="hljs-literal">()</span>);
spoutConfig.scheme = <span class="hljs-keyword">new</span> <span class="hljs-type">SchemeAsMultiScheme</span>(<span class="hljs-keyword">new</span> <span class="hljs-type">StringScheme</span><span class="hljs-literal">()</span>);
<span class="hljs-type">KafkaSpout</span> kafkaSpout = <span class="hljs-keyword">new</span> <span class="hljs-type">KafkaSpout</span>(spoutConfig);</code></pre>

<p>Trident Spout</p>

</div><div id="wmd-preview-section-300" class="wmd-preview-section preview-content">

<pre class="prettyprint hljs-dark"><code class="hljs ocaml"><span class="hljs-type">TridentTopology</span> topology = <span class="hljs-keyword">new</span> <span class="hljs-type">TridentTopology</span><span class="hljs-literal">()</span>;
<span class="hljs-type">BrokerHosts</span> zk = <span class="hljs-keyword">new</span> <span class="hljs-type">ZkHosts</span>(<span class="hljs-string">"localhost"</span>);
<span class="hljs-type">TridentKafkaConfig</span> spoutConf = <span class="hljs-keyword">new</span> <span class="hljs-type">TridentKafkaConfig</span>(zk, <span class="hljs-string">"test-topic"</span>);
spoutConf.scheme = <span class="hljs-keyword">new</span> <span class="hljs-type">SchemeAsMultiScheme</span>(<span class="hljs-keyword">new</span> <span class="hljs-type">StringScheme</span><span class="hljs-literal">()</span>);
<span class="hljs-type">OpaqueTridentKafkaSpout</span> spout = <span class="hljs-keyword">new</span> <span class="hljs-type">OpaqueTridentKafkaSpout</span>(spoutConf);</code></pre>

</div><div id="wmd-preview-section-301" class="wmd-preview-section preview-content">

<h2 id="二当拓扑出错时如何从上一次的kafka位置继续处理消息">（二）当拓扑出错时，如何从上一次的kafka位置继续处理消息</h2>

<p>1、KafkaConfig有一个配置项为KafkaConfig.startOffsetTime，它用于指定拓扑从哪个位置上开始处理消息，可取的值有3个： <br>
（1）kafka.api.OffsetRequest.EarliestTime(): 从最早的消息开始 <br>
（2）kafka.api.OffsetRequest.LatestTime(): 从最新的消息开始，即从队列队伍最末端开始。 <br>
（3）根据时间点: </p>

</div><div id="wmd-preview-section-302" class="wmd-preview-section preview-content">

<pre class="prettyprint hljs-dark"><code class="hljs ocaml">kafkaConfig.startOffsetTime =  <span class="hljs-keyword">new</span> <span class="hljs-type">SimpleDateFormat</span>(<span class="hljs-string">"yyyy.MM.dd-HH:mm:ss"</span>).parse(startOffsetTime).getTime<span class="hljs-literal">()</span>;</code></pre>

<p>可以参阅 How do I accurately get offsets of messages for a certain timestamp using OffsetRequest? 的实现原理。 <br>
How do I accurately get offsets of messages for a certain timestamp using OffsetRequest? <br>
 Kafka allows querying offsets of messages by time and it does so at segment granularity.The timestamp parameter is the unix timestamp and querying the offset by timestamp returns the latest possible offset of the message that is appended no later than the given timestamp. There are 2 special values of the timestamp - latest and earliest. For any other value of the unix timestamp, Kafka will get the starting offset of the log segment that is created no later than the given timestamp. Due to this, and since the offset request is served only at segment granularity, the offset fetch request returns less accurate results for larger segment sizes. <br>
 For more accurate results, you may configure the log segment size based on time (log.roll.ms) instead of size (log.segment.bytes). However care should be taken since doing so might increase the number of file handlers due to frequent log segment rolling. <br>
2、由于运行拓扑时，指定了offset在zk中保存的位置，当出现错误时，可以找出offset <br>
当重新部署拓扑时，必须保证offset的保存位置不变，它才能正确的读取到offset。 <br>
（1）对于core storm,就是</p>

</div><div id="wmd-preview-section-303" class="wmd-preview-section preview-content">

<pre class="prettyprint hljs-dark"><code class="hljs actionscript">SpoutConfigspoutConf = <span class="hljs-keyword">new</span> SpoutConfig(brokerHosts,topic, zkRoot,id);</code></pre>

<p>后2个参数不能变化 <br>
（2）对于trident而言，就是</p>

</div><div id="wmd-preview-section-304" class="wmd-preview-section preview-content">

<pre class="prettyprint hljs-dark"><code class="hljs 1c">topology.newStream(“test<span class="hljs-string">", kafkaSpout).</span></code></pre>

<p>第1个参数不能变化。 <br>
 3、也就是说只要拓扑运行过一次KafkaConfig.startOffsetTime，之后重新部署时均可从offset中开始。 <br>
再看看这2个参数</p>

</div><div id="wmd-preview-section-305" class="wmd-preview-section preview-content">

<pre class="prettyprint hljs-dark"><code class="hljs aspectj">   <span class="hljs-keyword">public</span> booleanforceFromStart =<span class="hljs-keyword">false</span>;
   <span class="hljs-keyword">public</span> <span class="hljs-keyword">long</span> startOffsetTime= kafka.api.OffsetRequest.EarliestTime();</code></pre>

<p>如果将forceFromStart(旧版本是ignoreZkOffsets）设置为true，则每次拓扑重新启动时，都会从开头读取消息。 <br>
如果为false，则： <br>
第一次启动，从开头读取，之后的重启均是从offset中读取。 <br>
一般使用时，将数值设置为以上2个即可。</p>

</div><div id="wmd-preview-section-306" class="wmd-preview-section preview-content">

<h2 id="三结果写回kafka">（三）结果写回kafka</h2>

<p>如果想把结果写回kafka，并保证事务性，可以使用 storm.kafka.trident.TridentState, storm.kafka.trident.TridentStateFactory and storm.kafka.trident.TridentKafkaUpdater.</p>

<p>以下是官方说明。 <br>
 Writing to Kafka as part of your topology <br>
You can create an instance of storm.kafka.bolt.KafkaBolt and attach it as a component to your topology or if you are using trident you can use storm.kafka.trident.TridentState, storm.kafka.trident.TridentStateFactory and storm.kafka.trident.TridentKafkaUpdater. <br>
You need to provide implementation of following 2 interfaces</p>

<p>TupleToKafkaMapper and TridentTupleToKafkaMapper <br>
These interfaces have 2 methods defined:</p>

</div><div id="wmd-preview-section-307" class="wmd-preview-section preview-content">

<pre class="prettyprint hljs-dark"><code class="hljs oxygene">    K getKeyFromTuple(<span class="hljs-keyword">Tuple</span>/TridentTuple <span class="hljs-keyword">tuple</span>);
    V getMessageFromTuple(<span class="hljs-keyword">Tuple</span>/TridentTuple <span class="hljs-keyword">tuple</span>);</code></pre>

<p>as the name suggests these methods are called to map a tuple to kafka key and kafka message. If you just want one field as key and one field as value then you can use the provided FieldNameBasedTupleToKafkaMapper.java implementation. In the KafkaBolt, the implementation always looks for a field with field name “key” and “message” if you use the default constructor to construct FieldNameBasedTupleToKafkaMapper for backward compatibility reasons. Alternatively you could also specify a different key and message field by using the non default constructor. In the TridentKafkaState you must specify what is the field name for key and message as there is no default constructor. These should be specified while constructing and instance of FieldNameBasedTupleToKafkaMapper.</p>

<p>KafkaTopicSelector and trident KafkaTopicSelector <br>
This interface has only one method</p>

</div><div id="wmd-preview-section-308" class="wmd-preview-section preview-content">

<pre class="prettyprint hljs-dark"><code class="hljs dust"><span class="xml">publicinterface KafkaTopicSelector </span><span class="hljs-expression">{
    <span class="hljs-variable">StringgetTopics</span>(<span class="hljs-variable">Tuple</span><span class="hljs-end-block">/TridentTuple tuple</span>);
}</span><span class="xml"></span></code></pre>

<p>The implementation of this interface should return topic to which the tuple’s key/message mapping needs to be published You can return a null and the message will be ignored. If you have one static topic name then you can use DefaultTopicSelector.java and set the name of the topic in the constructor.</p>

<p>Specifying kafka producer properties <br>
You can provide all the produce properties , see <a href="http://kafka.apache.org/documentation.html#producerconfigs" target="_blank">http://kafka.apache.org/documentation.html#producerconfigs</a> section “Important configuration properties for the producer”, in your storm topology config by setting the properties map with key kafka.broker.properties.</p>

<p>附带2个官方的示例 <br>
For the bolt :</p>

</div><div id="wmd-preview-section-309" class="wmd-preview-section preview-content">

<pre class="prettyprint hljs-dark"><code class="hljs vim">TopologyBuilder builder = <span class="hljs-keyword">new</span> TopologyBuilder();

        Fields fields = <span class="hljs-keyword">new</span> Fields(<span class="hljs-string">"key"</span>, <span class="hljs-string">"message"</span>);
        FixedBatchSpout spout = <span class="hljs-keyword">new</span> FixedBatchSpout(fields, <span class="hljs-number">4</span>,
                    <span class="hljs-keyword">new</span> Values(<span class="hljs-string">"storm"</span>, <span class="hljs-string">"1"</span>),
                    <span class="hljs-keyword">new</span> Values(<span class="hljs-string">"trident"</span>, <span class="hljs-string">"1"</span>),
                    <span class="hljs-keyword">new</span> Values(<span class="hljs-string">"needs"</span>, <span class="hljs-string">"1"</span>),
                    <span class="hljs-keyword">new</span> Values(<span class="hljs-string">"javadoc"</span>, <span class="hljs-string">"1"</span>)
        );
        spout.setCycle(true);
        builder.setSpout(<span class="hljs-string">"spout"</span>, spout, <span class="hljs-number">5</span>);
        KafkaBolt bolt = <span class="hljs-keyword">new</span> KafkaBolt()
                .withTopicSelector(<span class="hljs-keyword">new</span> DefaultTopicSelector(<span class="hljs-string">"test"</span>))
                .withTupleToKafkaMapper(<span class="hljs-keyword">new</span> FieldNameBasedTupleToKafkaMapper());
        builder.setBolt(<span class="hljs-string">"forwardToKafka"</span>, bolt, <span class="hljs-number">8</span>).shuffleGrouping(<span class="hljs-string">"spout"</span>);

        Config <span class="hljs-keyword">conf</span> = <span class="hljs-keyword">new</span> Config();
        //<span class="hljs-keyword">set</span> producer properties.
        Properties props = <span class="hljs-keyword">new</span> Properties();
        props.<span class="hljs-keyword">put</span>(<span class="hljs-string">"metadata.broker.list"</span>, <span class="hljs-string">"localhost:9092"</span>);
        props.<span class="hljs-keyword">put</span>(<span class="hljs-string">"request.required.acks"</span>, <span class="hljs-string">"1"</span>);
        props.<span class="hljs-keyword">put</span>(<span class="hljs-string">"serializer.class"</span>, <span class="hljs-string">"kafka.serializer.StringEncoder"</span>);
        <span class="hljs-keyword">conf</span>.<span class="hljs-keyword">put</span>(KafkaBolt.KAFKA_BROKER_PROPERTIES, props);

        StormSubmitter.submitTopology(<span class="hljs-string">"kafkaboltTest"</span>, <span class="hljs-keyword">conf</span>, builder.createTopology());</code></pre>

<p>For Trident:</p>

</div><div id="wmd-preview-section-310" class="wmd-preview-section preview-content">

<pre class="prettyprint hljs-dark"><code class="hljs vim">Fields fields = <span class="hljs-keyword">new</span> Fields(<span class="hljs-string">"word"</span>, <span class="hljs-string">"count"</span>);
        FixedBatchSpout spout = <span class="hljs-keyword">new</span> FixedBatchSpout(fields, <span class="hljs-number">4</span>,
                <span class="hljs-keyword">new</span> Values(<span class="hljs-string">"storm"</span>, <span class="hljs-string">"1"</span>),
                <span class="hljs-keyword">new</span> Values(<span class="hljs-string">"trident"</span>, <span class="hljs-string">"1"</span>),
                <span class="hljs-keyword">new</span> Values(<span class="hljs-string">"needs"</span>, <span class="hljs-string">"1"</span>),
                <span class="hljs-keyword">new</span> Values(<span class="hljs-string">"javadoc"</span>, <span class="hljs-string">"1"</span>)
        );
        spout.setCycle(true);

        TridentTopology topology = <span class="hljs-keyword">new</span> TridentTopology();
        Stream stream = topology.newStream(<span class="hljs-string">"spout1"</span>, spout);

        TridentKafkaStateFactory stateFactory = <span class="hljs-keyword">new</span> TridentKafkaStateFactory()
                .withKafkaTopicSelector(<span class="hljs-keyword">new</span> DefaultTopicSelector(<span class="hljs-string">"test"</span>))
                .withTridentTupleToKafkaMapper(<span class="hljs-keyword">new</span> FieldNameBasedTupleToKafkaMapper(<span class="hljs-string">"word"</span>, <span class="hljs-string">"count"</span>));
        stream.partitionPersist(stateFactory, fields, <span class="hljs-keyword">new</span> TridentKafkaUpdater(), <span class="hljs-keyword">new</span> Fields());

        Config <span class="hljs-keyword">conf</span> = <span class="hljs-keyword">new</span> Config();
        //<span class="hljs-keyword">set</span> producer properties.
        Properties props = <span class="hljs-keyword">new</span> Properties();
        props.<span class="hljs-keyword">put</span>(<span class="hljs-string">"metadata.broker.list"</span>, <span class="hljs-string">"localhost:9092"</span>);
        props.<span class="hljs-keyword">put</span>(<span class="hljs-string">"request.required.acks"</span>, <span class="hljs-string">"1"</span>);
        props.<span class="hljs-keyword">put</span>(<span class="hljs-string">"serializer.class"</span>, <span class="hljs-string">"kafka.serializer.StringEncoder"</span>);
        <span class="hljs-keyword">conf</span>.<span class="hljs-keyword">put</span>(TridentKafkaState.KAFKA_BROKER_PROPERTIES, props);
        StormSubmitter.submitTopology(<span class="hljs-string">"kafkaTridentTest"</span>, <span class="hljs-keyword">conf</span>, topology.build());</code></pre>

</div><div id="wmd-preview-section-311" class="wmd-preview-section preview-content">

<h1 id="二完整示例">二、完整示例</h1>

</div><div id="wmd-preview-section-312" class="wmd-preview-section preview-content">

<h2 id="一简介">（一）简介</h2>

<p>1、本项目主要完成以下功能： <br>
（1）从kafka中读取一个topic的消息，然后根据空格拆分单词，最后统计数据后写入一个HazelCastState（一个分布式的内存存储框架）。 <br>
（2）通过DRPC从上述的HazelCastState中读取结果，并将结果输出。 <br>
2、代码可分为3部分： <br>
（1）单词拆分 <br>
（2）定义拓扑行为 <br>
（3）state定义 <br>
以下分为三部分分别介绍。</p>

</div><div id="wmd-preview-section-313" class="wmd-preview-section preview-content">

<h2 id="二单词拆分">（二）单词拆分</h2>

<p>原理很简单，就是通过空格将单词进行拆分。</p>

</div><div id="wmd-preview-section-314" class="wmd-preview-section preview-content">

<pre class="prettyprint hljs-dark"><code class="hljs scala">public <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">WordSplit</span> <span class="hljs-keyword"><span class="hljs-keyword">extends</span></span> <span class="hljs-title">BaseFunction</span> {</span>
    public void execute(<span class="hljs-type">TridentTuple</span> tuple, <span class="hljs-type">TridentCollector</span> collector) {
        <span class="hljs-type">String</span> sentence = (<span class="hljs-type">String</span>) tuple.getValue(<span class="hljs-number">0</span>);
        <span class="hljs-keyword">if</span> (sentence != <span class="hljs-literal">null</span>) {
            sentence = sentence.replaceAll(<span class="hljs-string">"\r"</span>, <span class="hljs-string">""</span>);
            sentence = sentence.replaceAll(<span class="hljs-string">"\n"</span>, <span class="hljs-string">""</span>);
            <span class="hljs-keyword">for</span> (<span class="hljs-type">String</span> word : sentence.split(<span class="hljs-string">" "</span>)) {
                collector.emit(<span class="hljs-keyword">new</span> <span class="hljs-type">Values</span>(word));
            }
        }
    }
}</code></pre>

<p>这里的wordsplit是一个function，它继承自BaseFunction，最后，它将拆分出来的单词逐个emit出去。</p>

</div><div id="wmd-preview-section-315" class="wmd-preview-section preview-content">

<h2 id="三定义拓扑行为">（三）定义拓扑行为</h2>

</div><div id="wmd-preview-section-316" class="wmd-preview-section preview-content">

<h3 id="1定义kafka的相关配置">1、定义kafka的相关配置</h3>

</div><div id="wmd-preview-section-317" class="wmd-preview-section preview-content">

<pre class="prettyprint hljs-dark"><code class="hljs ocaml"> <span class="hljs-type">TridentKafkaConfig</span> kafkaConfig = <span class="hljs-keyword">new</span> <span class="hljs-type">TridentKafkaConfig</span>(brokerHosts, <span class="hljs-string">"storm-sentence"</span>, <span class="hljs-string">"storm"</span>);
        kafkaConfig.scheme = <span class="hljs-keyword">new</span> <span class="hljs-type">SchemeAsMultiScheme</span>(<span class="hljs-keyword">new</span> <span class="hljs-type">StringScheme</span><span class="hljs-literal">()</span>);
        <span class="hljs-type">TransactionalTridentKafkaSpout</span> kafkaSpout = <span class="hljs-keyword">new</span> <span class="hljs-type">TransactionalTridentKafkaSpout</span>(kafkaConfig); </code></pre>

<p>（1）首先定义一个kafka相关的配置对象，第一个参数是zookeeper的位置，第二个参数是订阅topic的名称，第三个参数是一个clientId <br>
（2）然后对配置进行一些设置，包括一些起始位置之类的，后面再补充具体的配置介绍。 <br>
（3）创建一个spout，这里的spout是事务型的，也就是保证每一个仅且只被处理一个</p>

</div><div id="wmd-preview-section-318" class="wmd-preview-section preview-content">

<h3 id="2定义拓扑进行单词统计后写入一个分布式内存中">2、定义拓扑，进行单词统计后，写入一个分布式内存中。</h3>

</div><div id="wmd-preview-section-319" class="wmd-preview-section preview-content">

<pre class="prettyprint hljs-dark"><code class="hljs ocaml"><span class="hljs-type">TridentTopology</span> topology= <span class="hljs-keyword">new</span> <span class="hljs-type">TridentTopology</span><span class="hljs-literal">()</span>;
<span class="hljs-type">TridentState</span> wordCounts = topology.newStream(<span class="hljs-string">"kafka"</span>, kafkaSpout).shuffle<span class="hljs-literal">()</span>.
                each(<span class="hljs-keyword">new</span> <span class="hljs-type">Fields</span>(<span class="hljs-string">"str"</span>), <span class="hljs-keyword">new</span> <span class="hljs-type">WordSplit</span><span class="hljs-literal">()</span>, <span class="hljs-keyword">new</span> <span class="hljs-type">Fields</span>(<span class="hljs-string">"word"</span>)).
                groupBy(<span class="hljs-keyword">new</span> <span class="hljs-type">Fields</span>(<span class="hljs-string">"word"</span>)).
                persistentAggregate(<span class="hljs-keyword">new</span> <span class="hljs-type">HazelCastStateFactory</span><span class="hljs-literal">()</span>, <span class="hljs-keyword">new</span> <span class="hljs-type">Count</span><span class="hljs-literal">()</span>, <span class="hljs-keyword">new</span> <span class="hljs-type">Fields</span>(<span class="hljs-string">"aggregates_words"</span>)).parallelismHint(<span class="hljs-number">2</span>);</code></pre>

<p>（1）创建一个topo。</p>

<p>（2）首先定义一个输入流，其中第一个参数定义了zk中放置这个topo元信息的信息，一般是/transactional/kafka <br>
（3）对每个输入的消息进行拆分：首先它的输入是字段名称为str的消息，然后经过WordSplit这个Function处理，最后，以字段名称word发送出去 <br>
（4）将结果根据word字段的值进行分组，就是说word值相同的放在一起。 <br>
（5）将分组的结果分别count一下，然后以字段名称aggregates_words写入HazelCastStateFactory定义的state中，关于state请见下一部分的介绍。</p>

</div><div id="wmd-preview-section-320" class="wmd-preview-section preview-content">

<h3 id="3从分布式内存中读取结果并进行输出">3、从分布式内存中读取结果并进行输出</h3>

</div><div id="wmd-preview-section-321" class="wmd-preview-section preview-content">

<pre class="prettyprint hljs-dark"><code class="hljs ocaml">topology.newDRPCStream(<span class="hljs-string">"words"</span>, drpc)
        .each(<span class="hljs-keyword">new</span> <span class="hljs-type">Fields</span>(<span class="hljs-string">"args"</span>), <span class="hljs-keyword">new</span> <span class="hljs-type">Split</span><span class="hljs-literal">()</span>, <span class="hljs-keyword">new</span> <span class="hljs-type">Fields</span>(<span class="hljs-string">"word"</span>))
        .groupBy(<span class="hljs-keyword">new</span> <span class="hljs-type">Fields</span>(<span class="hljs-string">"word"</span>))
        .stateQuery(wordCounts, <span class="hljs-keyword">new</span> <span class="hljs-type">Fields</span>(<span class="hljs-string">"word"</span>), <span class="hljs-keyword">new</span> <span class="hljs-type">MapGet</span><span class="hljs-literal">()</span>, <span class="hljs-keyword">new</span> <span class="hljs-type">Fields</span>(<span class="hljs-string">"count"</span>))
        .each(<span class="hljs-keyword">new</span> <span class="hljs-type">Fields</span>(<span class="hljs-string">"count"</span>), <span class="hljs-keyword">new</span> <span class="hljs-type">FilterNull</span><span class="hljs-literal">()</span>)
        .aggregate(<span class="hljs-keyword">new</span> <span class="hljs-type">Fields</span>(<span class="hljs-string">"count"</span>), <span class="hljs-keyword">new</span> <span class="hljs-type">Sum</span><span class="hljs-literal">()</span>, <span class="hljs-keyword">new</span> <span class="hljs-type">Fields</span>(<span class="hljs-string">"sum"</span>));</code></pre>

<p>（1）第三行定义了使用drpc需要处理的内容 <br>
（2）查询分布式内存中的内容，查询字段为word，然后以字段名count发送出去。 <br>
（3）将不需要统计的过滤掉。 <br>
（4）将结果进行聚合。</p>

<p>4、主函数</p>

</div><div id="wmd-preview-section-322" class="wmd-preview-section preview-content">

<pre class="prettyprint hljs-dark"><code class="hljs processing"><span class="hljs-keyword">String</span> kafkaZk = args[<span class="hljs-number">0</span>];
 SentenceAggregationTopology sentenceAggregationTopology = <span class="hljs-keyword">new</span> SentenceAggregationTopology(kafkaZk);
        Config config = <span class="hljs-keyword">new</span> Config();
        config.put(Config.TOPOLOGY_TRIDENT_BATCH_EMIT_INTERVAL_MILLIS, <span class="hljs-number">2000</span>);

        <span class="hljs-keyword">if</span> (args != <span class="hljs-keyword">null</span> &amp;&amp; args.length &gt; <span class="hljs-number">1</span>) {
            <span class="hljs-keyword">String</span> name = args[<span class="hljs-number">1</span>];
            <span class="hljs-keyword">String</span> dockerIp = args[<span class="hljs-number">2</span>];
            config.setNumWorkers(<span class="hljs-number">2</span>);
            config.setMaxTaskParallelism(<span class="hljs-number">5</span>);
            config.put(Config.NIMBUS_HOST, dockerIp);
            config.put(Config.NIMBUS_THRIFT_PORT, <span class="hljs-number">6627</span>);
            config.put(Config.STORM_ZOOKEEPER_PORT, <span class="hljs-number">2181</span>);
            config.put(Config.STORM_ZOOKEEPER_SERVERS, Arrays.asList(dockerIp));
            StormSubmitter.submitTopology(name, config, sentenceAggregationTopology.buildTopology());
        } <span class="hljs-keyword">else</span> {
            LocalDRPC drpc = <span class="hljs-keyword">new</span> LocalDRPC();
            config.setNumWorkers(<span class="hljs-number">2</span>);
            config.setMaxTaskParallelism(<span class="hljs-number">2</span>);
            LocalCluster cluster = <span class="hljs-keyword">new</span> LocalCluster();
            cluster.submitTopology(<span class="hljs-string">"kafka"</span>, config, sentenceAggregationTopology.buildTopology(drpc));
            <span class="hljs-keyword">while</span> (<span class="hljs-keyword">true</span>) {
                System.out.<span class="hljs-built_in">println</span>(<span class="hljs-string">"Word count: "</span> + drpc.execute(<span class="hljs-string">"words"</span>, <span class="hljs-string">"the"</span>));
                Utils.sleep(<span class="hljs-number">1000</span>);
            }

        }</code></pre>

<p>三个参数的含义为： </p>

<p>/*args[0]:kafkazk，如：192.168.172.98:2181,192.168.172.111:2181,192.168.172.114:2181,192.168.172.116:2181,192.168.172.117:2181/kafka <br>
     * args[1]:topo名称 <br>
     * args[2]:niubus节点，如,192.168.172.98 <br>
     */ <br>
当参数数据大于1时，将拓扑提交到集群中，否则提交到本地。提交拓扑到集群的比较直观，下面郑重介绍一下drpc的查询。 <br>
（1）首先定义一个本地的drpc对象，以及一个本地storm集群。 <br>
（2）然后将拓扑群提交到本地集群。 <br>
（3）最后，使用drpuc不停的循环查询统计结果并输出。</p>

<p>注意上面的拓扑定义了2个流，第一个流用于接收kafka消息，然后拆分统计后写入内存，第二个流则接受drpc的输入，将drpc的输入拆分后，再统计需要查询的每个单词的统计结果。如在本例中，需要显示单词the的数量。 <br>
在本例中，drpc和kafka没有本质的区别，它们都是一个用于向storm发送消息的集群，只是输入数据的方式有些不同，kafka通过spout输入，drpc则直接通过execute()进行输入。</p>

<p>运行方式： <br>
方式一：直接在eclipse右键运行，参数只填一个，如192.168.172.98:2181,192.168.172.111:2181,192.168.172.114:2181,192.168.172.116:2181,192.168.172.117:2181/kafka。只要保证kafka集群中有对应的topic，则会得到以下输出：</p>

<p>Word count: [[2]] <br>
Word count: [[5]] <br>
Word count: [[10]] <br>
Word count: [[17]] <br>
Word count: [[28]] <br>
当然，统计结果根据输入kafka的内容而不同。</p>

</div><div id="wmd-preview-section-323" class="wmd-preview-section preview-content">

<h2 id="四state定义">（四）state定义</h2>

<p>在定义拓扑的时候，最终的wordcount结果写在了HazelCastState中： <br>
persistentAggregate(new HazelCastStateFactory(),new Count(),new Fields(“aggregates_words”)) <br>
下面我们分析一下如何使用state来保存topo的处理结果，或者是中间处理结果。 <br>
注意，使用state除了可以保存最终的结果输出，以保证事务型、透明事务型以外，还经常用于保存中间结果。比如blueprint第3章的一个例子中，用于统计疾病的发生数量，如果超过预警值，则向外发信息。如果统计结果成功，但向外发送信息失败，则spout会重发数据，导致统计结果有误，因此，此时可以通过state将结果保存下来。</p>

</div><div id="wmd-preview-section-324" class="wmd-preview-section preview-content">

<h3 id="1factory类">1、Factory类</h3>

</div><div id="wmd-preview-section-325" class="wmd-preview-section preview-content">

<pre class="prettyprint hljs-dark"><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">HazelCastStateFactory</span> <span class="hljs-keyword">implements</span> <span class="hljs-title">StateFactory</span> </span>{
    <span class="hljs-annotation">@Override</span>
    <span class="hljs-function"><span class="hljs-keyword">public</span> State <span class="hljs-title">makeState</span><span class="hljs-params">(Map conf, IMetricsContext metrics, <span class="hljs-keyword">int</span> partitionIndex, <span class="hljs-keyword">int</span> numPartitions)</span> </span>{
        <span class="hljs-keyword">return</span> TransactionalMap.build(<span class="hljs-keyword">new</span> HazelCastState(<span class="hljs-keyword">new</span> HazelCastHandler()));
    }
}</code></pre>

<p>内容很简单，就是返回一个state，它也是三个state相关的类中唯一对外的接口。</p>

<p>2、Handler类</p>

</div><div id="wmd-preview-section-326" class="wmd-preview-section preview-content">

<pre class="prettyprint hljs-dark"><code class="hljs dart">public <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">HazelCastHandler</span> <span class="hljs-keyword">implements</span> <span class="hljs-title">Serializable</span> </span>{
    private transient <span class="hljs-built_in">Map</span>&lt;<span class="hljs-built_in">String</span>, Long&gt; state;
    public <span class="hljs-built_in">Map</span>&lt;<span class="hljs-built_in">String</span>, Long&gt; getState() {
        <span class="hljs-keyword">if</span> (state == <span class="hljs-keyword">null</span>) {
            state = Hazelcast.newHazelcastInstance().getMap(<span class="hljs-string">"state"</span>);
        }
        <span class="hljs-keyword">return</span> state;
    }</code></pre>

<p>使用单例模式返回一个map。</p>

</div><div id="wmd-preview-section-327" class="wmd-preview-section preview-content">

<h3 id="3state类">3、State类</h3>

<p>真正处理业务逻辑的类。主要的方法有mutiPut和mutiGet，用于将结果放入state与取出state。</p>

</div><div id="wmd-preview-section-328" class="wmd-preview-section preview-content">

<pre class="prettyprint hljs-dark"><code class="hljs nimrod">public class <span class="hljs-type">HazelCastState</span>&lt;T&gt; implements <span class="hljs-type">IBackingMap</span>&lt;<span class="hljs-type">TransactionalValue</span>&lt;<span class="hljs-type">Long</span>&gt;&gt; {

    private <span class="hljs-type">HazelCastHandler</span> handler;


    public <span class="hljs-type">HazelCastState</span>(<span class="hljs-type">HazelCastHandler</span> handler) {
        this.handler = handler;
    }

    public <span class="hljs-type">void</span> addKeyValue(<span class="hljs-type">String</span> key, <span class="hljs-type">Long</span> value) {
        <span class="hljs-type">Map</span>&lt;<span class="hljs-type">String</span>, <span class="hljs-type">Long</span>&gt; state = handler.getState();
        state.put(key, value);
    }

    @<span class="hljs-type">Override</span>
    public <span class="hljs-type">String</span> toString() {
        <span class="hljs-keyword">return</span> handler.getState().toString();
    }


    @<span class="hljs-type">Override</span>
    public <span class="hljs-type">void</span> multiPut(<span class="hljs-type">List</span>&lt;<span class="hljs-type">List</span>&lt;<span class="hljs-type">Object</span>&gt;&gt; keys, <span class="hljs-type">List</span>&lt;<span class="hljs-type">TransactionalValue</span>&lt;<span class="hljs-type">Long</span>&gt;&gt; vals) {
        <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>; i &lt; keys.size(); i++) {
            <span class="hljs-type">TridentTuple</span> key = (<span class="hljs-type">TridentTuple</span>) keys.get(i);
            <span class="hljs-type">Long</span> value = vals.get(i).getVal();
            addKeyValue(key.getString(<span class="hljs-number">0</span>), value);
            //<span class="hljs-type">System</span>.<span class="hljs-keyword">out</span>.println(<span class="hljs-string">"["</span> + key.getString(<span class="hljs-number">0</span>) + <span class="hljs-string">" - "</span> + value + <span class="hljs-string">"]"</span>);
        }
    }


    public <span class="hljs-type">List</span> multiGet(<span class="hljs-type">List</span>&lt;<span class="hljs-type">List</span>&lt;<span class="hljs-type">Object</span>&gt;&gt; keys) {
        <span class="hljs-type">List</span>&lt;<span class="hljs-type">TransactionalValue</span>&lt;<span class="hljs-type">Long</span>&gt;&gt; <span class="hljs-literal">result</span> = new <span class="hljs-type">ArrayList</span>&lt;<span class="hljs-type">TransactionalValue</span>&lt;<span class="hljs-type">Long</span>&gt;&gt;(keys.size());
        <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>; i &lt; keys.size(); i++) {
            <span class="hljs-type">TridentTuple</span> key = (<span class="hljs-type">TridentTuple</span>) keys.get(i);
            <span class="hljs-literal">result</span>.add(new <span class="hljs-type">TransactionalValue</span>&lt;<span class="hljs-type">Long</span>&gt;(<span class="hljs-number">0</span>L, <span class="hljs-type">MapUtils</span>.getLong(handler.getState(), key.getString(<span class="hljs-number">0</span>), <span class="hljs-number">0</span>L)));
        }
        <span class="hljs-keyword">return</span> <span class="hljs-literal">result</span>;
    }
}</code></pre></div><div id="wmd-preview-section-footnotes" class="preview-content"></div></div></body></html>