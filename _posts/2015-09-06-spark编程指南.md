---
layout: post
tile:  "spark编程指南"
date:  2015-09-06 14:56:42
categories: storm 大数据 
excerpt: spark编程指南
---

* content
{:toc}


 
[TOC]

参考：

英文：https://spark.apache.org/docs/latest/programming-guide.html

中文：http://www.cnblogs.com/lujinhong2/p/4651025.html 1.2.1版本的

 

#（一）快速入门

老规矩，先看一个简单示例，有个认识。这个示例来自官方example的SparkPi：
		
	package org.lujinhong.demo.spark
	
	/*
	 * 官方的sparkPi示例
	 */
	
	import scala.math.random
	
	import org.apache.spark._
	
	/** Computes an approximation to pi */
	object SparkPi {
	  def main(args: Array[String]) {
	    val conf = new SparkConf().setAppName("Spark Pi").setMaster("local")
	    val spark = new SparkContext(conf)
	    val slices = if (args.length > 0) args(0).toInt else 2
	    val n = math.min(100000L * slices, Int.MaxValue).toInt // avoid overflow
	    val count = spark.parallelize(1 until n, slices).map { i =>
	      val x = random * 2 - 1
	      val y = random * 2 - 1
	      if (x*x + y*y < 1) 1 else 0
	    }.reduce(_ + _)
	    println("Pi is roughly " + 4.0 * count / n)
	    spark.stop()
	  }
	
	}
注意以上的setMaster(“local”)是自己加上去的，方便直接在本地运行。如果在集群上运行，则通过spark-submit的—master参数指定。

写好代码后，就可以直接在eclipse中右键—>运行了。
